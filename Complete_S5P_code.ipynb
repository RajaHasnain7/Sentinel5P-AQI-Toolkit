{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK5TKAZKY9rc"
      },
      "source": [
        "# Mount to Collab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzDhphdrYP13"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfU4PWgfYpKh"
      },
      "source": [
        "# Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haIXAOffO5it"
      },
      "outputs": [],
      "source": [
        "# Utilities\n",
        "!pip install sentinelhub\n",
        "!pip install speedtest-cli # Install speedtest-cli\n",
        "!pip install termcolor\n",
        "!pip install geopandas\n",
        "!pip install xarray\n",
        "!pip install cartopy\n",
        "\n",
        "try:\n",
        "    import speedtest # Import the speedtest module to check internet speed\n",
        "    import os\n",
        "    import pickle # to export attributes dictionary from harpconversion.py to the main collab notebook\n",
        "    import datetime # for making the folder based on the dates\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "    import requests\n",
        "    import geopandas # to handle gdf\n",
        "    from datetime import datetime # for date parameter\n",
        "    from termcolor import colored # to have colored text\n",
        "    from multiprocessing.pool import ThreadPool # for downloading Products in parallel\n",
        "    from functools import partial # for downloading Products in parallel\n",
        "    import multiprocessing # to check the number of cores in collab\n",
        "    import zipfile # to extract zip files\n",
        "    from glob import iglob # data access in file manager\n",
        "    from os.path import join # same\n",
        "    from tqdm import tqdm\n",
        "    import xarray as xr\n",
        "    import cartopy # improved visualizations\n",
        "    import cartopy.crs as ccrs # Projected visualizations\n",
        "    import cartopy.feature as cf\n",
        "    from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
        "    import matplotlib.patches as mpatches\n",
        "    from datetime import timedelta\n",
        "    import time\n",
        "    import re\n",
        "\n",
        "\n",
        "    from sentinelhub import (SHConfig, DataCollection, SentinelHubCatalog, SentinelHubRequest, BBox, bbox_to_dimensions, CRS, MimeType, Geometry)\n",
        "except ModuleNotFoundError as e:\n",
        "    print(colored(f'Module import error: {e.name} not found', 'red'))\n",
        "else:\n",
        "    print(colored('\\nAll libraries properly loaded. Ready to start!!!', 'green'), '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xdR1TPjYxVN"
      },
      "source": [
        "# Downloading L2 S5P data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0tPuI_8qsy9"
      },
      "source": [
        "First we Enter the Search Details:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpnlWCHbqqLB"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content')\n",
        "from init import (\n",
        "    username,\n",
        "    password,\n",
        "    L2_Files,\n",
        "    L3_Files,\n",
        "    Base_Merged_Files,\n",
        "    Base_Final_Files,\n",
        "    Base_PBLH_T2M_Files,\n",
        "    start_date,\n",
        "    end_date,\n",
        "    pollutant,\n",
        "    data_collection,\n",
        "    aoi_file_path,\n",
        "    aoi_name_to_display,\n",
        "    qa,\n",
        "    unit,\n",
        "    num_threads,\n",
        "    num_workers\n",
        ")\n",
        "from utils import (\n",
        "    load_geojson,\n",
        "    extract_aoi,\n",
        "    analyze_pollutants,\n",
        "    move_nc_files_and_cleanup,\n",
        "    compute_lengths_and_offsets,\n",
        "    get_user_choice_to_filter_data,\n",
        "    filter_data,\n",
        "    count_pollutant_data_types,\n",
        "    get_keycloak,\n",
        "    refresh_access_token,\n",
        "    is_refresh_token_expired,\n",
        "    is_access_token_expired,\n",
        "    get_regex_pattern,\n",
        "    get_variables_to_include\n",
        "\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxiVidubPuVi"
      },
      "source": [
        "These are all the collections available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gy00lhhPmeY"
      },
      "outputs": [],
      "source": [
        "print(colored(\"These are all the available collections:\"),'green')\n",
        "for collection in DataCollection.get_available_collections():\n",
        "    print(collection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLFagIJPP3i7"
      },
      "source": [
        "The collection we need is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K86zs7HsPxmB"
      },
      "outputs": [],
      "source": [
        "print(colored(\"The collection we need is as follows:\"),'green')\n",
        "DataCollection.SENTINEL5P"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ndam-5deN3r"
      },
      "source": [
        "We can extract AOI form a geojson file using this code\n",
        "(which handles polygon and multipolygon geometries):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aTwy3WldAZ6"
      },
      "outputs": [],
      "source": [
        "# aoi_file_path is the actual path to your GeoJSON file\n",
        "geojson_path = aoi_file_path\n",
        "geojson_data = load_geojson(geojson_path)\n",
        "\n",
        "if geojson_data:\n",
        "    aoi_wkt = extract_aoi(geojson_data)\n",
        "    if aoi_wkt:\n",
        "        # Use the extracted AOI WKT string in your code\n",
        "        # For example, replacing a manual AOI string in an API request\n",
        "        aoi = aoi_wkt\n",
        "        print(\"Extracting AOI\")\n",
        "    else:\n",
        "        print(\"Failed to extract AOI.\")\n",
        "else:\n",
        "    print(\"Failed to load GeoJSON data.\")\n",
        "\n",
        "# In order to make the AOI format readable by the API we tweak the AOI a bit\n",
        "aoi=aoi + \"'\"\n",
        "aoi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-3pRy-_eLPt"
      },
      "source": [
        "Now we make the pollutant search:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZlLgN6W17KM"
      },
      "outputs": [],
      "source": [
        "start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
        "end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
        "\n",
        "# Initialize an empty list to store results dataframes\n",
        "all_results = []\n",
        "\n",
        "# Iterate through each day in the date range\n",
        "current_date = start_date\n",
        "while current_date <= end_date:\n",
        "    # Construct the date string in the required format\n",
        "    current_date_str = current_date.strftime('%Y-%m-%d')\n",
        "\n",
        "    # Construct the query URL for the current date\n",
        "    query_url = f\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=Collection/Name eq '{data_collection}' and contains(Name,'{pollutant}') and OData.CSC.Intersects(area=geography'SRID=4326;{aoi}) and ContentDate/Start ge {current_date_str}T00:00:00.000Z and ContentDate/Start lt {current_date_str}T23:59:59.999Z\"\n",
        "\n",
        "    # Make the request for the current date\n",
        "    json = requests.get(query_url).json()\n",
        "\n",
        "    # Check if there are any results for the current date\n",
        "    if 'value' in json:\n",
        "        # Convert the results to a dataframe and append to the list\n",
        "        results_df = pd.DataFrame.from_dict(json['value'])\n",
        "        all_results.append(results_df)\n",
        "\n",
        "    # Move to the next day\n",
        "    current_date += timedelta(days=1)\n",
        "\n",
        "# Concatenate all dataframes in the list into a single dataframe\n",
        "results = pd.concat(all_results, ignore_index=True)\n",
        "\n",
        "# Output the final dataframe\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMDMzaY1nZ0h"
      },
      "source": [
        "We can also filter the pollutant based on its type e.g (Offline, Real-time, Reprocessed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B91H_2chrvcx"
      },
      "outputs": [],
      "source": [
        "# Call the function and get counts and total\n",
        "offline_count, nrti_count, rpro_count, total_count = count_pollutant_data_types(results)\n",
        "\n",
        "# Print the counts and total\n",
        "print(f\"Total number of entries: {total_count}\")\n",
        "print(f\"Number of Offline data entries: {offline_count}\")\n",
        "print(f\"Number of Near Real-Time data entries: {nrti_count}\")\n",
        "print(f\"Number of Reprocessed data entries: {rpro_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpB7No0ompDr"
      },
      "outputs": [],
      "source": [
        "# Get a valid user choice and the corresponding base_type\n",
        "user_choice, base_type = get_user_choice_to_filter_data()\n",
        "\n",
        "# Display the selected base_type\n",
        "print(f\"Base type selected: {base_type}\")\n",
        "\n",
        "# Filter the DataFrame based on the user's choice\n",
        "results = filter_data(results, user_choice)\n",
        "\n",
        "# Output or further process the filtered DataFrame\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFp59zLmOrz8"
      },
      "source": [
        "Check the downloading details:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJZrzMw1l2wc"
      },
      "outputs": [],
      "source": [
        "analyze_pollutants(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A230tj3Q0Q77"
      },
      "source": [
        "Code to download the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPJvCwAjFWdN"
      },
      "outputs": [],
      "source": [
        "# Setup for obtaining keycloak token and refresh token\n",
        "keycloak_token, refresh_token = get_keycloak(username, password)\n",
        "token_acquired_time = datetime.now()  # Store the time when the token was acquired\n",
        "\n",
        "# Your time constraint setup remains the same\n",
        "current_time = datetime.now()\n",
        "time_change = pd.DateOffset(minutes=8)\n",
        "expiration_time = current_time + time_change\n",
        "\n",
        "# Get the results dataset\n",
        "id_series = results['Id']\n",
        "\n",
        "# Placeholder for file dates (You'll need to adjust how you obtain these dates)\n",
        "file_dates = [start_date, end_date]  # these dates are taken from the dates which the user entered\n",
        "\n",
        "# Find min and max dates to determine the range\n",
        "\n",
        "min_date = min(file_dates)\n",
        "\n",
        "max_date = max(file_dates)\n",
        "\n",
        "# Generate folder name based on the date range\n",
        "folder_name = f\"{pollutant}({min_date.strftime('%B')} - {max_date.strftime('%B')}){min_date.year}\"\n",
        "\n",
        "# Define the base folder path where you want to save the files\n",
        "base_folder_path = L2_Files #For drive, give the path to drive base folder here\n",
        "\n",
        "# Combine the base folder path with the generated folder name\n",
        "folder_path = os.path.join(base_folder_path, folder_name)\n",
        "\n",
        "# Ensure the target folder exists\n",
        "os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "# Print statements for the user\n",
        "print(\"Total .zip files to be downloaded:\",len(id_series))\n",
        "print(\"Folder path where files will be saved:\",folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9q-4KNP95V8"
      },
      "outputs": [],
      "source": [
        "# Your setup to download the data\n",
        "session = requests.Session()\n",
        "session.headers.update({'Authorization': f'Bearer {keycloak_token}'})\n",
        "\n",
        "max_retry_attempts = 3  # Maximum number of retry attempts for each download\n",
        "\n",
        "for index, product_id in enumerate(id_series, start=1):  # Start counting from 1\n",
        "    attempt = 0\n",
        "    download_success = False\n",
        "\n",
        "    while attempt < max_retry_attempts and not download_success:\n",
        "      try:\n",
        "          current_time = datetime.now()\n",
        "          print(f\"Attempt {attempt + 1}: Downloading {index} of {len(id_series)}: {product_id}\")\n",
        "          print(\"Current time: \", current_time)\n",
        "\n",
        "          # Check if the refresh token has expired before attempting to refresh the access token\n",
        "          if is_refresh_token_expired(token_acquired_time):\n",
        "              print(\"Refresh token expired, re-authenticating...\")\n",
        "              keycloak_token, refresh_token = get_keycloak(username, password)\n",
        "              token_acquired_time = datetime.now()  # Update the token acquired time\n",
        "              session.headers.update({'Authorization': f'Bearer {keycloak_token}'})  # Update the session's token\n",
        "              expiration_time = current_time + pd.DateOffset(minutes=8)  # Reset the expiration time for the access token\n",
        "              print(\"New access token expiration time: \", expiration_time)\n",
        "          elif current_time >= expiration_time:\n",
        "              # If only the access token has expired, refresh it\n",
        "              print(\"Refreshing access token...\")\n",
        "              keycloak_token = refresh_access_token(refresh_token)\n",
        "              session.headers.update({'Authorization': f'Bearer {keycloak_token}'})  # Update the session's token\n",
        "              expiration_time = current_time + pd.DateOffset(minutes=8)  # Reset the expiration time for the access token\n",
        "              print(\"New access token expiration time: \", expiration_time)\n",
        "\n",
        "          # Proceed to download the data\n",
        "          url = f\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products({product_id})/$value\"\n",
        "          response = session.get(url, allow_redirects=False)\n",
        "\n",
        "          # Follow redirects if necessary\n",
        "          while response.status_code in (301, 302, 303, 307):\n",
        "              url = response.headers['Location']\n",
        "              response = session.get(url, allow_redirects=True)\n",
        "\n",
        "          # Download the file\n",
        "          if response.ok:\n",
        "              file_path = os.path.join(folder_path, f\"{product_id}.zip\")\n",
        "              with open(file_path, 'wb') as file:\n",
        "                  file.write(response.content)\n",
        "              print(f\"Successfully downloaded {product_id}\")\n",
        "              download_success = True\n",
        "          else:\n",
        "              print(f\"Failed to download {product_id}. HTTP Status: {response.status_code}\")\n",
        "              attempt += 1\n",
        "              time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "\n",
        "      except requests.exceptions.RequestException as e:\n",
        "          print(f\"Network error on attempt {attempt + 1} for {product_id}: {e}\")\n",
        "          attempt += 1\n",
        "          time.sleep(5)  # Wait for 5 seconds before retrying\n",
        "\n",
        "    if not download_success:\n",
        "            print(f\"Failed to download {product_id} after {max_retry_attempts} attempts.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgLrND2uS4na"
      },
      "source": [
        "Code to delete corrupt files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FufZT9IES9KS"
      },
      "outputs": [],
      "source": [
        "# Define the base folder path\n",
        "base_folder_path = L2_Files\n",
        "\n",
        "# Define the minimum file size threshold in bytes (5 MB in this case)\n",
        "min_file_size_bytes = 5 * 1024 * 1024  # 5 MB converted to bytes\n",
        "\n",
        "# Initialize counters for the number of files deleted and the total files examined\n",
        "deleted_files_count = 0\n",
        "total_files_examined = 0\n",
        "\n",
        "# Iterate through all files in the base folder\n",
        "for folder_name, subfolders, filenames in os.walk(base_folder_path):\n",
        "    for filename in filenames:\n",
        "        # Construct the full file path\n",
        "        file_path = os.path.join(folder_name, filename)\n",
        "\n",
        "        # Check if the file is a .zip file\n",
        "        if file_path.endswith('.zip'):\n",
        "            total_files_examined += 1  # Increment the total files examined counter\n",
        "            # Get the size of the file\n",
        "            file_size = os.path.getsize(file_path)\n",
        "\n",
        "            # Check if the file size is less than the minimum threshold\n",
        "            if file_size < min_file_size_bytes:\n",
        "                # Delete the file\n",
        "                os.remove(file_path)\n",
        "                deleted_files_count += 1  # Increment the deleted files counter\n",
        "                print(f\"Deleted {file_path} due to insufficient size.\")\n",
        "\n",
        "# Print the total number of files deleted and the total files examined\n",
        "print(f\"Total number of .zip files examined: {total_files_examined}\")\n",
        "print(f\"Total number of files deleted due to being under 5MB: {deleted_files_count}\")\n",
        "print(f\"Deleted {deleted_files_count} out of {total_files_examined} files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C4b8mjARNbq"
      },
      "source": [
        "code to check if all the files have been downloaded:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdMwOQo3RMen"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty list to hold IDs of files that weren't downloaded\n",
        "missing_files = []\n",
        "\n",
        "for product_id in id_series:\n",
        "    # Construct the expected file path for each product ID\n",
        "    expected_file_path = os.path.join(folder_path, f\"{product_id}.zip\")\n",
        "\n",
        "    # Check if the file exists\n",
        "    if not os.path.exists(expected_file_path):\n",
        "        # If the file doesn't exist, add its ID to the list of missing files\n",
        "        missing_files.append(product_id)\n",
        "    else:\n",
        "        print(f\"File for product {product_id} exists.\")\n",
        "\n",
        "# Update id_series to include only the IDs of files that weren't downloaded\n",
        "id_series = missing_files\n",
        "\n",
        "if not id_series:\n",
        "    print(\"All files have been successfully downloaded.\")\n",
        "else:\n",
        "    print(f\"Files missing for {len(id_series)} products. Need to download again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In case there are missing files which need to be downloaded again, run the **\"Your setup to download the data\"** cell again"
      ],
      "metadata": {
        "id": "s_-YYRH28fb0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXvL2JAK040F"
      },
      "source": [
        "# Unzipping the files and keeping only NetCDF files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is to extract all the zip files to get .nc files"
      ],
      "metadata": {
        "id": "3LhqUzvz83pi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYyEtj7ETpwg"
      },
      "outputs": [],
      "source": [
        "#Code to check and remount drive for connection issues\n",
        "def check_and_remount_drive():\n",
        "    if not os.path.ismount('/content/drive'):\n",
        "        print(\"Drive is not mounted. Attempting to remount...\")\n",
        "        try:\n",
        "            drive.mount('/content/drive')\n",
        "        except ValueError:\n",
        "            # Force unmount and remount if the mount fails\n",
        "            print(\"Mount failed. Attempting to unmount and remount...\")\n",
        "            !fusermount -u /content/drive\n",
        "            drive.mount('/content/drive')\n",
        "    else:\n",
        "        print(\"Drive is already mounted.\")\n",
        "\n",
        "# Function to extract zip files with retries\n",
        "def extract_zip_with_retry(zip_path, extract_to, retries=3, delay=10):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_to)\n",
        "            return True\n",
        "        except zipfile.BadZipFile:\n",
        "            print(colored(f'Error extracting: {zip_path} - Not a valid zip file', 'red'))\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            print(colored(f'Error on attempt {attempt + 1} - retrying in {delay} seconds: {e}', 'yellow'))\n",
        "            #check_and_remount_drive()  # Remount Google Drive if necessary\n",
        "            time.sleep(delay)\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pI1WzUKgTjQQ"
      },
      "outputs": [],
      "source": [
        "folder_path_to_extract = folder_path\n",
        "total_files_examined = 0\n",
        "extracted_files_count = 0\n",
        "\n",
        "for file in os.listdir(folder_path_to_extract):\n",
        "    if file.endswith('.zip'):\n",
        "        total_files_examined += 1\n",
        "        zip_path = os.path.join(folder_path_to_extract, file)\n",
        "        if extract_zip_with_retry(zip_path, folder_path_to_extract):\n",
        "            print(colored(f'Extracted {total_files_examined}: {file}', 'green'))\n",
        "            extracted_files_count += 1\n",
        "\n",
        "print(f\"Extracted {extracted_files_count} out of {total_files_examined}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CTwvClOaKpR"
      },
      "source": [
        "This code is to cleanup the drive folder and only keep .nc files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ0lxq9iY3E9"
      },
      "outputs": [],
      "source": [
        "# Specify the root directory\n",
        "root_directory = folder_path_to_extract\n",
        "move_nc_files_and_cleanup(root_directory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1OLMoDoq9D0"
      },
      "source": [
        "# Variables for harp file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_0-H-7Yq9D7"
      },
      "outputs": [],
      "source": [
        "#make a requirments file to check package requirements\n",
        "!pip freeze > requirements_for_downloading_S5P_data.txt\n",
        "\n",
        "# Define your variables\n",
        "qa_for_harp=qa\n",
        "unit_for_harp=unit\n",
        "aoi_for_harp=aoi_file_path\n",
        "pollutant_for_harp=pollutant\n",
        "L2_files_for_harp= folder_path_to_extract\n",
        "# Now we create a path to a new folder for L3_Files\n",
        "# Find min and max dates to determine the range\n",
        "min_date = min(file_dates)\n",
        "max_date = max(file_dates)\n",
        "# Generate folder name based on the date range\n",
        "folder_name = f\"{pollutant}({min_date.strftime('%B')} - {max_date.strftime('%B')}){min_date.year}\"\n",
        "folder_name = folder_name.replace(\"L2\", \"L3\")\n",
        "# Combine the base folder path with the generated folder name\n",
        "L3_files_for_harp = os.path.join(L3_Files, folder_name)\n",
        "# Ensure the target folder exists\n",
        "os.makedirs(L3_files_for_harp, exist_ok=True)\n",
        "\n",
        "gdf_for_harp = geopandas.read_file(aoi_for_harp)\n",
        "minx, miny, maxx, maxy = gdf_for_harp.total_bounds # Use total_bounds to get the overall bounding box\n",
        "\n",
        "resolution = (0.01, 0.01)  #Step size for spatial re-gridding (in degrees)\n",
        "# Compute offsets and number of samples using the resolution\n",
        "xstep, ystep = resolution\n",
        "\n",
        "lat_length, lat_offset, lon_length, lon_offset = compute_lengths_and_offsets(minx, miny, maxx, maxy, ystep, xstep)\n",
        "\n",
        "# Specify the file path for variables.py\n",
        "file_path = \"variables_for_harp.py\"\n",
        "\n",
        "# Write the variables to variables.py\n",
        "with open(file_path, \"w\") as f:\n",
        "    f.write(f'qa_for_harp = {qa_for_harp}\\n')\n",
        "    f.write(f'unit_for_harp = \"{unit_for_harp}\"\\n')\n",
        "    f.write(f'aoi_for_harp = \"{aoi_for_harp}\"\\n')\n",
        "    f.write(f'pollutant_for_harp = \"{pollutant_for_harp}\"\\n')\n",
        "    f.write(f'L2_files_for_harp = \"{L2_files_for_harp}\"\\n')\n",
        "    f.write(f'L3_files_for_harp = \"{L3_files_for_harp}\"\\n')\n",
        "    f.write(f'lat_length = {lat_length}\\n')\n",
        "    f.write(f'lat_offset = {lat_offset}\\n')\n",
        "    f.write(f'lon_length = {lon_length}\\n')\n",
        "    f.write(f'lon_offset = {lon_offset}\\n')\n",
        "    f.write(f'xstep = {xstep}\\n')\n",
        "    f.write(f'ystep = {ystep}\\n')\n",
        "    #f.write(f'gdf_for_harp = {gdf_for_harp}\\n')\n",
        "\n",
        "print(colored(\"Variables written to variables_for_harp.py\",'green'))\n",
        "print(colored(\"Requirements for downloading data written to requierments_for_downloading_S5P_data.txt\",'green'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwFimy6Alj6r"
      },
      "source": [
        "# Conversion from L2 to L3 using Harp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCuCl67e3YUZ"
      },
      "source": [
        "First we install Miniconda:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vP3vOVRRjFYx"
      },
      "outputs": [],
      "source": [
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh\n",
        "!bash /tmp/miniconda.sh -bfp /usr/local\n",
        "import os\n",
        "os.environ['PATH'] = \"/usr/local/bin:\" + os.environ['PATH']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjbF6LLG3h9G"
      },
      "source": [
        "Then we set-up the conda virtual environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDiQAF9nkyo8"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "eval \"$(conda shell.bash hook)\" # copy conda command to shell\n",
        "conda env list\n",
        "conda env remove -n myenv -y\n",
        "conda create --override-channels -c conda-forge -c stcorp-forge --name myenv -y python=3.12.7\n",
        "conda activate myenv\n",
        "conda env list\n",
        "python --version\n",
        "conda install -c conda-forge harp -y # Preprocess L2 to L3 S5p data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jGOvUb17ip4"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "eval \"$(conda shell.bash hook)\" # copy conda command to shell\n",
        "conda env list\n",
        "conda activate myenv\n",
        "conda env list\n",
        "python --version\n",
        "#sudo pip3 uninstall matplotlib\n",
        "sudo apt-get install python3-matplotlib\n",
        "pip install netCDF4\n",
        "pip install basemap\n",
        "conda install matplotlib -y\n",
        "conda install cartopy -y\n",
        "conda install xarray -y\n",
        "conda install numpy -y\n",
        "conda install pandas -y\n",
        "conda install imageio -y\n",
        "conda install termcolor -y\n",
        "conda install conda-forge::tqdm -y\n",
        "conda install anaconda::requests -y\n",
        "conda install conda-forge::speedtest-cli -y\n",
        "pip install geopandas -y\n",
        "conda install anaconda::ipywidgets -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JdWQkKl4FxG"
      },
      "source": [
        "Finally we check if all the necessary packages are installed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsuIKZeSuVCt"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "eval \"$(conda shell.bash hook)\" # copy conda command to shell\n",
        "conda env list\n",
        "conda activate myenv\n",
        "conda env list\n",
        "python --version\n",
        "# use pip list and conda list to check all the packages\n",
        "conda list | grep -E 'matplotlib|netCDF4|basemap|cartopy|xarray|numpy|pandas|imageio|termcolor|tqdm|requests|speedtest-cli|geopandas|ipywidgets'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYWpltXpm2y3"
      },
      "source": [
        "Finally we run the harp.py file for L2-L3 conversion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4a6ktV60m1u_"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "eval \"$(conda shell.bash hook)\" # copy conda command to shell\n",
        "conda env list\n",
        "conda activate myenv\n",
        "\n",
        "python harpconversion.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-aT4Wt91fKm"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Specify the path to your .pkl file\n",
        "file_path = '/content/attributes.pkl'\n",
        "\n",
        "# Use the files.download() function to download the file to your local system\n",
        "files.download(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yAzClAxGgk7"
      },
      "source": [
        "# L3-CSV conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BvHSiZHJ9OE"
      },
      "outputs": [],
      "source": [
        "# Replace 'L2' with 'L3' and assign to a new variable\n",
        "pollutant_for_L3 =  pollutant.replace(\"L2\", \"L3\")\n",
        "\n",
        "\n",
        "# Directory containing the .nc files\n",
        "L3_files_directory = L3_files_for_harp # Replace with the actual path\n",
        "\n",
        "# Generate the regex pattern\n",
        "date_regex = get_regex_pattern(base_type, pollutant_for_L3)\n",
        "\n",
        "# List all .nc files\n",
        "L3_nc_files = [f for f in os.listdir(L3_files_directory) if f.endswith('.nc')]\n",
        "\n",
        "# Check which files do not match the regex pattern\n",
        "for f in L3_nc_files:\n",
        "    if not re.search(date_regex, f):\n",
        "        print(f\"No match: {f}\")\n",
        "\n",
        "# Filter files that match the date pattern\n",
        "matching_files = [f for f in L3_nc_files if re.search(date_regex, f)]\n",
        "\n",
        "# Extract dates from filenames and sort them\n",
        "L3_nc_files_sorted = sorted(\n",
        "    matching_files,\n",
        "    key=lambda x: re.search(date_regex, x).group(1)\n",
        ")\n",
        "\n",
        "L3_nc_files_sorted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_jEPBqNd58S"
      },
      "source": [
        "Then we check the total number of files we have"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJVmW7nBKqpX"
      },
      "outputs": [],
      "source": [
        "len(L3_nc_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOkEc-wAKxL7"
      },
      "outputs": [],
      "source": [
        "len(L3_nc_files_sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSq57nC_bdXJ"
      },
      "source": [
        "if the L3_nc_files are equal to the L3_nc_files_sorted then we move forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMwhcnbdG6mY"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty DataFrame to hold all the data\n",
        "combined_data = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzVPyeQzdJtm"
      },
      "source": [
        "In the next block we are preparing the CSV file path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_o93PeeNe_s"
      },
      "outputs": [],
      "source": [
        "# Format the dates\n",
        "min_date_str = min_date.strftime(\"%B%d\")  # July01\n",
        "max_date_str = max_date.strftime(\"%B%d\")  # December31\n",
        "\n",
        "\n",
        "# Construct the filename\n",
        "L3_CSV_filename = f\"{pollutant_for_L3}({min_date_str}-{max_date_str}).csv\"\n",
        "\n",
        "base_folder_path_L3_CSV = L3_Files #For drive, give the path to drive base folder here\n",
        "\n",
        "# Combine the base folder path with the generated folder name\n",
        "file_path_L3_CSV = os.path.join(base_folder_path_L3_CSV, L3_CSV_filename)\n",
        "\n",
        "print(f\"Path to new csv file: {file_path_L3_CSV}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFQXFWxWG9Y6"
      },
      "outputs": [],
      "source": [
        "for file_name in L3_nc_files_sorted:\n",
        "    file_path = os.path.join(L3_files_directory, file_name)\n",
        "    # Open the .nc file\n",
        "    ds = xr.open_dataset(file_path)\n",
        "\n",
        "    # Assuming you want to extract the same variables as before\n",
        "    # For NO2\n",
        "    #variables_to_include = ['tropospheric_NO2_column_number_density', 'cloud_fraction']\n",
        "    # For CO\n",
        "    #variables_to_include = ['CO_column_number_density']\n",
        "    # For O3\n",
        "    #variables_to_include = ['O3_column_number_density','O3_effective_temperature','cloud_fraction']\n",
        "    # For Aerosol\n",
        "    #variables_to_include = ['absorbing_aerosol_index']\n",
        "    # For SO2\n",
        "    #variables_to_include = ['SO2_column_number_density','cloud_fraction']\n",
        "\n",
        "    variables_to_include = get_variables_to_include(pollutant_for_L3)\n",
        "\n",
        "    df = ds[variables_to_include].to_dataframe().reset_index()\n",
        "\n",
        "    # Extract date from file_name using the regex\n",
        "    file_date = re.search(date_regex, file_name).group(1)\n",
        "    # Convert string date to datetime format\n",
        "    df['date'] = pd.to_datetime(file_date, format='%Y%m%d')\n",
        "\n",
        "    # Append the extracted data to the combined DataFrame\n",
        "    combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
        "\n",
        "# Once all files are processed, sort by date if not already sorted\n",
        "combined_data.sort_values(by='date', inplace=True)\n",
        "\n",
        "# Convert the time dimension if present and format it according to your needs\n",
        "# Note: This part is omitted here but can be included based on the specific time formatting you require\n",
        "\n",
        "# Export the combined dataset to CSV\n",
        "combined_csv_path = file_path_L3_CSV\n",
        "combined_data.to_csv(combined_csv_path, index=False)\n",
        "print(f\"Combined CSV file saved to: {combined_csv_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBDaesKyTpqx"
      },
      "source": [
        "# Remove NAN values from CSV using spline interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have to manually change column names here.\n",
        "\n",
        "e.g :\n",
        "\n",
        "1.   \"tropospheric_NO2_column_number_density\" for NO2\n",
        "2.   \"CO_column_number_density\" for CO\n",
        "\n"
      ],
      "metadata": {
        "id": "L5AFMI3BJ4Ao"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsFxJPX7TsLX"
      },
      "outputs": [],
      "source": [
        "def fill_nan_values_and_flag(csv_file_path):\n",
        "\n",
        "    # Load the CSV file\n",
        "    data = pd.read_csv(csv_file_path)\n",
        "\n",
        "    # Create a new column 'Is_Interpolated' and set it to 'No' initially\n",
        "    data['Is_Interpolated'] = 'No'\n",
        "\n",
        "    # Identify rows with NaN values in the specified column\n",
        "    nan_rows_before = data['CO_column_number_density'].isna()\n",
        "\n",
        "    # Fill NaN values using cubic spline interpolation\n",
        "    data['CO_column_number_density'] = data['CO_column_number_density'].interpolate(method='spline', order=3)\n",
        "\n",
        "    # Identify rows that were filled by checking against the original NaN flags\n",
        "    nan_rows_after = data['CO_column_number_density'].isna()\n",
        "    # Rows that were NaN before but not after were interpolated\n",
        "    interpolated_rows = nan_rows_before & ~nan_rows_after\n",
        "    data.loc[interpolated_rows, 'Is_Interpolated'] = 'Yes'\n",
        "\n",
        "    # Group data by date and check for complete NaNs in each group\n",
        "    complete_nan_dates = data.groupby('date')['CO_column_number_density'].apply(lambda x: x.isna().all())\n",
        "\n",
        "    # Filter dates where all values are NaN\n",
        "    complete_nan_dates = complete_nan_dates[complete_nan_dates].index.tolist()\n",
        "\n",
        "    print(\"Dates with completely missing 'CO_column_number_density' values:\")\n",
        "    for date in complete_nan_dates:\n",
        "        print(f\"- {date}\")\n",
        "\n",
        "    # Optionally, save the filled and flagged dataset back to a CSV\n",
        "    filled_csv_file_path = csv_file_path.replace('.csv', '_filled_flagged.csv')\n",
        "    data.to_csv(filled_csv_file_path, index=False)\n",
        "\n",
        "    print(f\"NaN values filled using spline interpolation and flagged. Filled dataset saved to {filled_csv_file_path}.\")\n",
        "\n",
        "# Example usage\n",
        "csv_file_path = file_path_L3_CSV  # Replace this with the path to your CSV file\n",
        "\n",
        "fill_nan_values_and_flag(csv_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw-S2jdJajt6"
      },
      "source": [
        "visualizing the spline interpolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TVrJlJ7YWA_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load your dataset\n",
        "file_path = file_path_L3_CSV\n",
        "\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Assuming 'tropospheric_NO2_column_number_density' has NaN values you've interpolated\n",
        "# Perform spline interpolation (for demonstration, using cubic spline, order=3)\n",
        "data['interpolated'] = data['CO_column_number_density'].interpolate(method='spline', order=3)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot original data points\n",
        "plt.scatter(data.index, data['CO_column_number_density'], color='blue', label='Original Data', alpha=0.5)\n",
        "\n",
        "# Plot interpolated data\n",
        "plt.plot(data.index, data['interpolated'], color='red', label='Spline Interpolation', linewidth=2)\n",
        "\n",
        "plt.title('Visual Inspection of Spline Interpolation Fit')\n",
        "plt.xlabel('Index or Time')\n",
        "plt.ylabel(f'{pollutant_for_L3} column number density')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rkgphk1lYXJZ"
      },
      "source": [
        "**Tips for Visual Inspection:**\n",
        "\n",
        "**Identify Overfitting:** Look for sections where the red spline line (interpolated data) follows the blue points (original data) too closely, including the noise. This might indicate overfitting.\n",
        "\n",
        "**Identify Underfitting:** Notice areas where the spline does not capture significant trends or patterns in the data, suggesting underfitting.\n",
        "\n",
        "**Adjust Order:** If you observe overfitting or underfitting, consider adjusting the order of the spline interpolation. Lower orders reduce complexity (less likely to overfit) but might miss capturing the data trend accurately (underfit). Higher orders can capture more complex patterns but risk overfitting to noise.\n",
        "\n",
        "This process is iterativeâ€”adjust the spline order and inspect the fit visually until you find the balance that best represents your data's underlying trend."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "fK5TKAZKY9rc",
        "cfU4PWgfYpKh",
        "i1OLMoDoq9D0"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}